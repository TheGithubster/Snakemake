Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	blast
	1

[Sun Sep 30 16:41:25 2018]
Job 0: <-- Starting blast -->

[Sun Sep 30 16:41:25 2018]
Error in rule blast:
    jobid: 0
    output: scripts/data/blast_results.out.txt, scripts/data/blast_results.complete-visual-svg.svg

RuleException:
CalledProcessError in line 23 of /home/thomas/Desktop/Snakemake/Snakefile:
Command ' set -euo pipefail;  perl scripts/ncbiblast_lwp.pl --email "Thom-rein97@hotmail.nl" --sequence services/sequence.fasta --database "uniprotkb_bacteria" --stype protein --program blastp --align 7 --alignments 100 --outfile scripts/data/blast_results ' returned non-zero exit status 2.
  File "/home/thomas/Desktop/Snakemake/Snakefile", line 23, in __rule_blast
  File "/home/thomas/miniconda3/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/thomas/Desktop/Snakemake/.snakemake/log/2018-09-30T164125.463333.snakemake.log
