Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	blast
	1	select_fasta
	2

[Sat Oct 20 15:59:02 2018]
Job 1: <-- Starting file stream from root -->

[Sat Oct 20 16:00:49 2018]
Finished job 1.
1 of 2 steps (50%) done

[Sat Oct 20 16:00:49 2018]
Job 0: <-- Starting blast -->

Terminating processes on user request, this might take some time.
[Sat Oct 20 16:01:02 2018]
Error in rule blast:
    jobid: 0
    output: scripts/data/blast_results.out.txt, scripts/data/blast_results.complete-visual-svg.svg

RuleException:
CalledProcessError in line 32 of /home/thomas/Desktop/Snakemake/Snakefile:
Command ' set -euo pipefail;  perl services/ncbiblast_lwp.pl --email "Thom-rein97@hotmail.nl" --sequence copy.fa --database "uniprotkb_bacteria" --stype protein --program blastp --align 7 --alignments 5 --outfile scripts/data/blast_results ' died with <Signals.SIGINT: 2>.
  File "/home/thomas/Desktop/Snakemake/Snakefile", line 32, in __rule_blast
  File "/home/thomas/miniconda3/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Complete log: /home/thomas/Desktop/Snakemake/.snakemake/log/2018-10-20T155902.915482.snakemake.log
